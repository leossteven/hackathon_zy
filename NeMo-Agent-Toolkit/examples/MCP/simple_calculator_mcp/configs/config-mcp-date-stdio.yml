# SPDX-FileCopyrightText: Copyright (c) 2024-2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This config file shows how to use the MCP server to get the current date and time.
# Here the workflow acts as a MCP client and connects to the MCP server running
# on the specified URL. streamable-http is the recommended transport for HTTP-based
# connections, but sse is also supported for backwards compatibility.

# This config file shows -
# 1. how to use a local MCP server to get the current date and time using stdio transport.
# 2. how to access a remote MCP server using streamable-http transport for math operations.
#
# As the mcp_server_time is running locally ensure that the package "mcp_server_time" is installed
# on your local machine. For example, if you are using pip, you can install it with:
# uv pip install mcp-server-time

general:
  use_uvloop: true

functions:
  mcp_time:
    _type: mcp_client
    server:
      transport: stdio
      command: "python"
      args: ["-m", "mcp_server_time", "--local-timezone=America/Los_Angeles"]
    # load 1 of 2 tools and override tool attributes
    tool_filter:
      get_current_time:
        alias: get_current_time_mcp_tool
        description: "Returns the current date and time from the MCP server"

  mcp_math:
    _type: mcp_client
    server:
      transport: streamable-http
      url: "http://localhost:9901/mcp"
    # load all tools and use the names and descriptions from the server

llms:
  nim_llm:
    _type: nim
    model_name: meta/llama-3.1-70b-instruct
    temperature: 0.0
    max_tokens: 1024
  openai_llm:
    _type: openai
    model_name: gpt-3.5-turbo
    max_tokens: 2000

workflow:
  _type: react_agent
  # List all tools that are needed for the workflow from the two MCP servers
  # get_current_time_mcp_tool is from the local mcp_server_time server
  # calculator_multiply, calculator_divide, calculator_subtract, calculator_inequality are from the remote mcp_server_math server
  tool_names: ["get_current_time_mcp_tool", "calculator_multiply", "calculator_divide", "calculator_subtract", "calculator_inequality"]
  llm_name: nim_llm
  verbose: true
  retry_parsing_errors: true
  max_retries: 3
